{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3e24be04",
      "metadata": {
        "id": "3e24be04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7096b5e5",
      "metadata": {
        "id": "7096b5e5",
        "outputId": "6e464b08-9ab7-4597-9e8a-36333c4256f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4c1fc75f",
      "metadata": {
        "id": "4c1fc75f"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen3-1.8B-Instruct\"  # use 4B if GPU is strong\n",
        "DATA_PATH = \"/content/drive/MyDrive/NLP/news.csv\"\n",
        "OUTPUT_DIR = \"/content/qwen3_burmese_headline\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "64b0c9d6",
      "metadata": {
        "id": "64b0c9d6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Headline Generator Dataset/headline_corpus.csv\")\n",
        "df.head(5)\n",
        "\n",
        "X = df[\"text\"]\n",
        "y = df['headline']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Split temp into validation (15%) and test (15%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Check sizes\n",
        "print(\"Train:\", len(X_train))\n",
        "print(\"Validation:\", len(X_val))\n",
        "print(\"Test:\", len(X_test))"
      ],
      "metadata": {
        "id": "OKknMNFKFiB7",
        "outputId": "85e13d8e-667d-4d30-9a40-78ae7adc294f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OKknMNFKFiB7",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 11853\n",
            "Validation: 2540\n",
            "Test: 2541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25117951",
      "metadata": {
        "id": "25117951"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ee44da",
      "metadata": {
        "id": "92ee44da"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3a6025",
      "metadata": {
        "id": "2c3a6025"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0875d955",
      "metadata": {
        "id": "0875d955"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "tokenized_ds = dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214e8d8d",
      "metadata": {
        "id": "214e8d8d"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcabaa4c",
      "metadata": {
        "id": "fcabaa4c"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}